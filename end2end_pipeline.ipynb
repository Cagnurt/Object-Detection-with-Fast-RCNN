{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b016827-16ae-415d-ab24-f523b08fc024",
   "metadata": {},
   "source": [
    "# End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4cfab-d2df-499b-888b-b5d8c06da52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src import getSystemInfo\n",
    "import os\n",
    "from constants import DATASETS\n",
    "import sys\n",
    "from dataset import BoltNutDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from src import video_to_img\n",
    "from dataset_analysis import  DatasetNumericalAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "from config import Config\n",
    "from model import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3ce72-6b8b-4d4b-8b69-9025aeb5f9d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hardware specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c6654-54d4-4729-9c1a-ef1728148508",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(getSystemInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07308c82-5b67-4524-a07c-7d6c3c421b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e35595-d100-4864-8472-50ff48ffb60d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing  \n",
    "Convert video frame into the image for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d56f6-c0b1-4a56-a0a5-48c85eaaa5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dt in DATASETS:\n",
    "#     video_to_img('/home/cagnur/stroma/dataset/images/'+dt+'/'+dt+'.mp4', '/home/cagnur/stroma/dataset/images/'+dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64944994-0014-4402-bcd5-16910b603b2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208571bc-de01-40df-9812-6578b5d3372f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Analysis: Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb13ef3-f874-4c02-8755-46562d191fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path = '/home/cagnur/stroma/dataset/annotations'\n",
    "analysis = DatasetNumericalAnalysis(annotation_path)\n",
    "analysis.vis_compare_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ebbbf-51fd-4858-b9b0-d4d1edd5689d",
   "metadata": {},
   "source": [
    "**Comment**: This dataset distribution is from ML Era. With deep learning era, this distribution change drasticaly.   \n",
    "**Conclusion:** We have less number of data compare to deep learning approaches (data size ~1M). We can use transfer learning to use advantages of deep learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb150e25-a570-441f-8765-e8d57503d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.vis_compare_subcategories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234c292-12f2-4631-9e82-a0a702cf03c0",
   "metadata": {},
   "source": [
    "**Comment 1 :** Validation and Test set should have same distribution. Otherwise it can explode.  \n",
    "**Comment 2 :** Becasue #nut is significantly less than #bolt, we need to apply data augmentation to balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04a997-ae4d-4504-81dc-49e334359e35",
   "metadata": {},
   "source": [
    "**Conclusion:** Good to go :) There is not any unknown category in the dataset. There is not any inconsistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae88b15-7840-45b8-9fb7-91df14455fbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Challange for Data Augmentation:  \n",
    "In order to increase the number of nut, we can extract nut pixels, apply rotation or etc, and add into random places of training images. If there was a segmentation info, this challange can be handled much more easily. However, we know bbox only. Extracting bbox and applying augmentation techniques and adding random places of images might hurt the training. Because there will be some background inconsistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4679af-ad23-462a-8ccf-f3c5e5627f67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Analysis: Computer Vision Point of View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396dd5a-1470-4dec-a438-e49c240bacee",
   "metadata": {},
   "source": [
    "Because there is a light condition in the problem definition, converting HLS channel might help, which converts the image into a hue, saturation, and lightness components instead of the RGB representation. Why? Because under different lightning conditions, H&S channels help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d6941-d8e8-48fa-bc30-5e445275af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(analysis.train_img_num, size=(1,)).item()\n",
    "    path = os.path.join('/home/cagnur/stroma/dataset/images/train/imgs',str(sample_idx).zfill(4)+'.jpg')\n",
    "    bgr_img = cv2.imread(path)\n",
    "    hls_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2HLS)\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(hls_img.reshape((640,640,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a620d8-eb08-4b53-b286-e26f60381082",
   "metadata": {},
   "source": [
    "In MyDataset class, I add HSL conversion. Becasue there is not torch transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577e7d8-238c-4e6f-b076-fe8fc4849fdc",
   "metadata": {},
   "source": [
    "Data augmentation is for increasing number of data in dataset, especially for imbalanced data. However, it is not the only reason. Sometimes we apply augmentation techniques for decreasing trainig time or to boost our model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7de87c-956d-49ab-b440-44cd63ac3a3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bbeea-f4a4-415a-a832-4c2160e37893",
   "metadata": {},
   "source": [
    "We can add many transformation as we can. Important thing is not all transformation is applied for validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a413e95-f73b-40f5-b6a0-427fb846262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate = transforms.RandomRotation(degrees=15)\n",
    "hFlip = transforms.RandomHorizontalFlip(p=0.25)\n",
    "vFlip = transforms.RandomVerticalFlip(p=0.25)\n",
    "trainTransforms = transforms.Compose([hFlip, vFlip, rotate,\n",
    "        transforms.ToTensor()])\n",
    "valTransforms = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3bdb1-7a15-407a-8825-67324aa858f3",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa165c-5492-4c49-9399-7f1092d8bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/cagnur/stroma/dataset/'\n",
    "video_path = os.path.join(data_path, 'images')\n",
    "ann_paths = os.path.join(data_path, 'annotations')\n",
    "\n",
    "for dt in DATASETS:\n",
    "    img_path = os.path.join(video_path, dt+'/imgs')\n",
    "    ann_path = os.path.join(ann_paths, 'instances_'+dt+'.json')\n",
    "    if dt == 'train':        \n",
    "        train_set = BoltNutDataset(img_path,ann_path, trainTransforms)\n",
    "        train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "    elif dt == 'val':\n",
    "        val_set = BoltNutDataset(img_path,ann_path, valTransforms)\n",
    "        val_loader = DataLoader(val_set, batch_size=4, shuffle=True)\n",
    "    elif dt == 'test':\n",
    "        test_set = BoltNutDataset(img_path,ann_path, valTransforms)\n",
    "        test_loader = DataLoader(test_set, batch_size=4, shuffle=True)\n",
    "    else:\n",
    "        print(\"Unknown!\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e446a-0429-4596-bb15-0bea2a9ec488",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although I added model.py which has several models, I will not use them for this challenge. The reason I want to add into the file is to show OOP's clarity and efficiency in programming. Also, this facilitates the debugging. (Clean code principles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "74256c80-e0c1-482c-bf8c-7893f6cd1ab3",
   "metadata": {},
   "source": [
    "I started with faster rcnn due to this paper: https://www.nature.com/articles/s41598-021-02805-y.pdf\n",
    "I could not write the model similar to models in model.py. I got error and I skipped because of time limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8bf76-9626-471a-9c70-5a4b96c6844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_faster_rcnn():\n",
    "    # load a model pre-trained on COCO\n",
    "    FasterRCNN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # replace the classifier with a new one, that has\n",
    "    # num_classes which is user-defined\n",
    "    num_classes = 2  # 1 class (person) + background\n",
    "    # get number of input features for the classifier\n",
    "    in_features = FasterRCNN.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    FasterRCNN.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8dd0d5-94ee-421d-b004-6eb5ad00a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca1185-5885-43da-93dd-1e7c93bcf186",
   "metadata": {},
   "source": [
    "These following functions are from one of my homeworks. In that hw, I applied these following functions to prune with different proportion rate. As a result, I reported effect's of two pruning methods by comparing such as proportion rate vs accuracy, cpu inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e58c22-a128-4a40-a0e8-8fed9289df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import prune\n",
    "\n",
    "def prune_model_l1_unstructured(model, layer_type, proportion):\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.l1_unstructured(module, 'weight', proportion)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "def prune_model_l1_structured(model, layer_type, proportion):\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.ln_structured(module, 'weight', proportion, n=1, dim=1)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "# Ex:\n",
    "# compressed_0p1_model =  prune_model_l1_unstructured(copy.deepcopy(uncompressed_model), torch.nn.Conv2d, 0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b0888-6b71-41bc-8097-b8296d2e7bf6",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f371d-d1d9-4976-81d4-989f773ec510",
   "metadata": {},
   "source": [
    "For this part, I use wandb :). Since I have student licence, I can use free. Epoch, optimizer and etc. are defined in this class. Also, I can define selections for hyperparameter search purposes. For example, in Config class, momentum and learning rates will sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f872fcc-85a8-4eed-8da0-9286c6881c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('faster')\n",
    "config.train_dataloader = train_loader\n",
    "config.valid_dataloader = val_loader\n",
    "config.test_dataloader = test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3a5bd-5a5f-43de-b8de-893815424236",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba2e9b-9f92-4873-b077-a76a61af79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(config, model):\n",
    "    sweep_id = wandb.sweep(config.sweep, entity = \"cagnur\", project=\"Stroma\")\n",
    "    \n",
    "    def train():\n",
    "        wandb.init()\n",
    "        # Training\n",
    "        if model == 'cnn':\n",
    "            config.model = Net(wandb.config.hidden_dim).cuda()\n",
    "        elif model == 'resnet':\n",
    "            config.model = ResNet(wandb.config.hidden_dim).cuda()\n",
    "        elif model == 'mlp':\n",
    "            config.model = MLP(wandb.config.hidden_dim).cuda()\n",
    "        elif model == 'efficient':\n",
    "            config.model = Efficient().cuda()\n",
    "        elif model == 'faster':\n",
    "            config.model = modified_faster_rcnn()\n",
    "        config.optimizer = torch.optim.SGD(config.model.parameters(), lr=wandb.config.learning_rate, momentum=wandb.config.momentum)\n",
    "        wandb.watch(config.model, config.criterion, log = 'all', log_freq = config.log_freq)\n",
    "        config.model.train()\n",
    "        counter = 0\n",
    "        for epoch in range(config.epoch):            \n",
    "            for imgs, labels in tqdm.tqdm(config.train_dataloader):\n",
    "                imgs, labels = imgs.cuda(), labels.cuda()\n",
    "                # imgs, labels = imgs, labels\n",
    "\n",
    "                out = config.model(imgs)\n",
    "                loss = config.criterion(out, labels)\n",
    "                config.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                config.optimizer.step()\n",
    "                counter += 1\n",
    "                if counter % 5 == 0:\n",
    "                    wandb.log({'Loss': loss}, step = counter)\n",
    "        # Training is done\n",
    "        # Validation\n",
    "        config.model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm.tqdm(config.test_dataloader):\n",
    "                imgs, labels = imgs.cuda(), labels.cuda()\n",
    "                # imgs, labels = imgs, labels\n",
    "                out = config.model(imgs)\n",
    "                predictions = out.argmax(dim=1, keepdim=True)  \n",
    "                correct += predictions.eq(labels.view_as(predictions)).sum().item()\n",
    "        accuracy = correct/len(config.valid_dataloader.dataset)\n",
    "        wandb.log({\"Accuracy\":accuracy} )\n",
    "        # Validation is done\n",
    "        # Export the model   \n",
    "        # torch.onnx.export(config.model,         # model being run \n",
    "        #                  imgs,     # model input (or a tuple for multiple inputs) \n",
    "        #                  \"model.onnx\",     # where to save the model  \n",
    "        #                  export_params=True # store the trained parameter weights inside the model file \n",
    "        #                  )\n",
    "        # wandb.save(\"model.onnx\")\n",
    "    wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954f30a-354c-4307-8b4a-bc91a8039f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67013c95-47d9-4c53-b636-902160fdafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(config, 'faster')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, becasue of the time limitation, I could not train and hyperparameter search since I had error in model part. But I believe that the practice I am trying to implement is clear and very easy to implement."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl] *",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
